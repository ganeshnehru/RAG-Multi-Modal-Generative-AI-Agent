{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:28.255137Z",
     "start_time": "2024-06-16T05:18:27.992928Z"
    }
   },
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:56.361441Z",
     "start_time": "2024-06-16T05:18:56.359276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_message = '''You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images. Here is your input: {image}\n",
    "'''"
   ],
   "id": "fcad527033419988",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:56.689040Z",
     "start_time": "2024-06-16T05:18:56.686520Z"
    }
   },
   "cell_type": "code",
   "source": "system_prompt = SystemMessagePromptTemplate.from_template(system_message)",
   "id": "a5bd9b0736e49a10",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:57.021869Z",
     "start_time": "2024-06-16T05:18:57.019190Z"
    }
   },
   "cell_type": "code",
   "source": "human_prompt = HumanMessagePromptTemplate.from_template('Carefully analyze this image describe the context of the photo.: {image}')",
   "id": "2ecc2108e77b6a41",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:57.367877Z",
     "start_time": "2024-06-16T05:18:57.365187Z"
    }
   },
   "cell_type": "code",
   "source": "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_prompt])",
   "id": "90c247883417f88f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:57.722278Z",
     "start_time": "2024-06-16T05:18:57.719487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\" )\n"
   ],
   "id": "ebd694f967c66722",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:58.161058Z",
     "start_time": "2024-06-16T05:18:58.158475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = chat, prompt = chat_prompt)"
   ],
   "id": "89805cb46be13979",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:18:59.120517Z",
     "start_time": "2024-06-16T05:18:59.116824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from temp_test import old_image_processor as ip"
   ],
   "id": "a339e03bab93ff9f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:19:01.249280Z",
     "start_time": "2024-06-16T05:18:59.448628Z"
    }
   },
   "cell_type": "code",
   "source": "image = ip.process_image('/Users/ganesh/Downloads/m_resized.jpg')",
   "id": "8158c63f7e7e90d2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:19:02.602541Z",
     "start_time": "2024-06-16T05:19:02.217412Z"
    }
   },
   "cell_type": "code",
   "source": "result = chain.run(image=image)",
   "id": "1348d0b04fdc0d48",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "[422] Unprocessable Entity\nbody\n  Value error, The roles of the messages must be alternating between `user` and `assistant` (type=value_error; error=The roles of the messages must be alternating between `user` and `assistant`)\nRequestID: 049db45a-626a-4bce-9a38-1a0f34f1776c",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:148\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     emit_warning()\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/base.py:605\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    601\u001B[0m         _output_key\n\u001B[1;32m    602\u001B[0m     ]\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m--> 605\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    606\u001B[0m         _output_key\n\u001B[1;32m    607\u001B[0m     ]\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    610\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    611\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    612\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but none were provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    613\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:148\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     emit_warning()\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/base.py:383\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    376\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    381\u001B[0m }\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/base.py:166\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    161\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    162\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/llm.py:126\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    123\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    124\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    125\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 126\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain/chains/llm.py:138\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    136\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[0;32m--> 138\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    145\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[1;32m    146\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[1;32m    147\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:599\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    592\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    593\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    597\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    598\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 599\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:456\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    455\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 456\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    457\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    458\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    460\u001B[0m ]\n\u001B[1;32m    461\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:446\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    445\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 446\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    447\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    448\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    452\u001B[0m         )\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:671\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 671\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    675\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:211\u001B[0m, in \u001B[0;36mChatNVIDIA._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    205\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m    210\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_custom_preprocess(messages)\n\u001B[0;32m--> 211\u001B[0m     responses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_callback_out(responses, run_manager)\n\u001B[1;32m    213\u001B[0m     message \u001B[38;5;241m=\u001B[39m ChatMessage(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_custom_postprocess(responses))\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:323\u001B[0m, in \u001B[0;36mChatNVIDIA._get_generation\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    321\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop)\n\u001B[1;32m    322\u001B[0m payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_payload(inputs\u001B[38;5;241m=\u001B[39minputs, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 323\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_req_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpayload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:380\u001B[0m, in \u001B[0;36mNVEModel.get_req_generation\u001B[0;34m(self, payload, invoke_url, stop)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Method for an end-to-end post query with NVE post-processing.\"\"\"\u001B[39;00m\n\u001B[1;32m    379\u001B[0m invoke_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_invoke_url(invoke_url)\n\u001B[0;32m--> 380\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_req\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minvoke_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m output, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(response, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:369\u001B[0m, in \u001B[0;36mNVEModel.get_req\u001B[0;34m(self, payload, invoke_url)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m payload\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    368\u001B[0m     payload \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n\u001B[0;32m--> 369\u001B[0m response, session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\u001B[43minvoke_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait(response, session)\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:214\u001B[0m, in \u001B[0;36mNVEModel._post\u001B[0;34m(self, invoke_url, payload)\u001B[0m\n\u001B[1;32m    210\u001B[0m session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_session_fn()\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_response \u001B[38;5;241m=\u001B[39m response \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__add_authorization(deepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_inputs))\n\u001B[1;32m    213\u001B[0m )\n\u001B[0;32m--> 214\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response, session\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:311\u001B[0m, in \u001B[0;36mNVEModel._try_raise\u001B[0;34m(self, response)\u001B[0m\n\u001B[1;32m    309\u001B[0m     body \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease check or regenerate your API key.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m# todo: raise as an HTTPError\u001B[39;00m\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheader\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mbody\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mException\u001B[0m: [422] Unprocessable Entity\nbody\n  Value error, The roles of the messages must be alternating between `user` and `assistant` (type=value_error; error=The roles of the messages must be alternating between `user` and `assistant`)\nRequestID: 049db45a-626a-4bce-9a38-1a0f34f1776c"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:14:49.044736Z",
     "start_time": "2024-06-16T05:14:49.038513Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "7cd7d09b115baad6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|assistant|><|assistant|> The image provided does not contain a chart. It appears to be a collection of random characters and symbols that do not form any recognizable pattern or data representation. Therefore,f,q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/v/z/q/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T23:15:38.498724Z",
     "start_time": "2024-06-15T23:15:38.495599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import base64\n",
    "# \n",
    "# def process_image(image_path):\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "#     return encoded_string\n",
    "\n",
    "from utils import old_image_processor as ip"
   ],
   "id": "9b1eeb563603086e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:05:15.177645Z",
     "start_time": "2024-06-16T05:05:12.826565Z"
    }
   },
   "cell_type": "code",
   "source": "image = ip.process_image('/Users/ganesh/Downloads/gimmelwald-switz (1).jpg')",
   "id": "5139671626405b23",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:07:46.407602Z",
     "start_time": "2024-06-16T05:07:20.078616Z"
    }
   },
   "cell_type": "code",
   "source": "result = chain.run(image=image)",
   "id": "123a11a0f4f5afe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ganesh/miniconda3/envs/vision/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:07:49.353166Z",
     "start_time": "2024-06-16T05:07:49.347481Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "dc63f27115c7a747",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This diagram is a representation of a protein structure. It is a 3D model of a protein. The protein is a transmembrane protein. The diagram shows the protein in its native state. The protein is composed of 200 amino acids. The protein has a transmembrane helix. The transmembrane helix is composed of 100 amino acids. The transmembrane helix is composed of 50 transmembrane helixs. The transmembrane helixs are composed of 20 transmembrane helixs. The transmembrane helixs are composed of 10 transmembrane helixs. The transmembrane helixs are composed of 5 transmembrane helixs. The transmembrane helixs are composed of 2 transmembrane helixs. The transmembrane helixs are composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane helix is composed of 1 transmembrane helix. The transmembrane helix is composed of 5 transmembrane helixs. The transmembrane'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:19:45.782793Z",
     "start_time": "2024-06-16T05:19:40.822127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "from utils import old_image_processor as ip\n",
    "\n",
    "# System message for the chat model\n",
    "system_message = '''You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images. Here is your input: {image}\n",
    "'''\n",
    "\n",
    "# Create System and Human message templates\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    'Carefully analyze this image and describe the context of the photo: {image}')\n",
    "\n",
    "# Combine system and human prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using the custom image processor utility\n",
    "image = ip.process_image('/Users/ganesh/Downloads/m_resized.jpg')\n",
    "\n",
    "# Run the chain with the processed image\n",
    "result = chain.run(image=image)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "e04a5d22e1fea12c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|assistant|><|assistant|> The image provided does not contain a chart. It appears to be a collection of various characters and symbols that do not form any recognizable chart or data visualization. The text seems to be a random assortment of letters, numbers, and symbols without any discernible pattern or structure that would suggest it is a chart or data representation. Therefore, I cannot provide an analysis or interpretation of a chart based on this image.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:21:46.415849Z",
     "start_time": "2024-06-16T05:21:41.380407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "from utils import old_image_processor as ip\n",
    "\n",
    "# System message for the chat model\n",
    "system_message = '''You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images. Here is your input: {image}\n",
    "'''\n",
    "\n",
    "# Create System and Human message templates\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    'Carefully analyze this image and describe the context of the photo: {image}')\n",
    "\n",
    "# Combine system and human prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using the custom image processor utility\n",
    "image = ip.process_image('/Users/ganesh/Downloads/m_resized.jpg')\n",
    "\n",
    "# Run the chain with the processed image\n",
    "result = chain.run(image=image)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "a8b411ab74cd4555",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|assistant|><|assistant|> The image provided does not contain a chart. It appears to be a collection of various characters and symbols that do not form any recognizable chart or data visualization. The text seems to be a random assortment of letters, numbers, and symbols without any discernible pattern or structure that would suggest it is a chart or data visualization. Therefore, I cannot provide a description of a chart based on this image.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "from utils import old_image_processor as ip\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the system message and human message templates\n",
    "system_message = \"You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images. Here is your input: <|image_1|>\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Carefully analyze this image and describe the context of the photo: <|image_1|>')\n",
    "\n",
    "# Combine prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using the custom image processor utility\n",
    "image = ip.process_image('/Users/ganesh/Downloads/m_resized.jpg')\n",
    "\n",
    "# Define the input prompt with image placeholder\n",
    "input_prompt = \"<|image_1|>\\nCarefully analyze this image and describe the context of the photo.\"\n",
    "\n",
    "# Prepare the input data for the model\n",
    "inputs = chat.processor.apply_chat_template([{\"role\": \"user\", \"content\": input_prompt}], images=[image], return_tensors=\"pt\")\n",
    "\n",
    "# Run the chain with the processed image\n",
    "result = chain.run(inputs=inputs)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "23fd93cb5420a017"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:24:09.497358Z",
     "start_time": "2024-06-16T05:24:09.492071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def process_image(image_path, max_size=180_000, max_tokens=8100):\n",
    "    \"\"\"\n",
    "    Process an input image to ensure the encoded image size is appropriate.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The file path to the input image.\n",
    "        max_size (int): The maximum size in bytes for the base64 encoded image.\n",
    "        max_tokens (int): The maximum number of tokens for the encoded image.\n",
    "\n",
    "    Returns:\n",
    "        str: The base64 encoded image string if within the size limit.\n",
    "        Raises ValueError if the image cannot be resized to fit within the limit.\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    # Open the image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        img = Image.open(img_file)\n",
    "\n",
    "        # Function to encode image to base64\n",
    "        def encode_image(image):\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        # Encode the image\n",
    "        encoded_image = encode_image(img)\n",
    "        encoded_image_tokens = encoding.encode(encoded_image)\n",
    "\n",
    "        # Check if the encoded image size is within the limit\n",
    "        if len(encoded_image) < max_size and len(encoded_image_tokens) < max_tokens:\n",
    "            return encoded_image\n",
    "\n",
    "        # Resize the image until it fits within the size limit\n",
    "        while len(encoded_image) >= max_size or len(encoded_image_tokens) >= max_tokens:\n",
    "            # Calculate the resize factor (reduce size by 10%)\n",
    "            new_width = int(img.width * 0.9)\n",
    "            new_height = int(img.height * 0.9)\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "            encoded_image = encode_image(img)\n",
    "            encoded_image_tokens = encoding.encode(encoded_image)\n",
    "\n",
    "        # Final check\n",
    "        if len(encoded_image) < max_size and len(encoded_image_tokens) < max_tokens:\n",
    "            return encoded_image\n",
    "        else:\n",
    "            raise ValueError(\"Image cannot be resized to fit within the size limit.\")\n",
    "\n",
    "# Usage Example:\n",
    "# image_path = \"/Users/ganesh/Downloads/imgs/class-diagram-example.png\"\n",
    "# try:\n",
    "#     encoded_image = process_image(image_path)\n",
    "#     print(\"Encoded image size is appropriate.\")\n",
    "# except ValueError as e:\n",
    "#     print(f\"Error: {e}\")\n"
   ],
   "id": "23b68db53f0098cf",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:25:22.817241Z",
     "start_time": "2024-06-16T05:25:20.984484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the system message and human message templates\n",
    "system_message = \"You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images. Here is your input: <|image_1|>\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Carefully analyze this image and describe the context of the photo: <|image_1|>')\n",
    "\n",
    "# Combine prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using the custom image processor utility\n",
    "image_path = ('/Users/ganesh/Downloads/m_resized.jpg')\n",
    "encoded_image = process_image(image_path)\n",
    "\n",
    "# Define the input prompt with image placeholder\n",
    "input_prompt = f\"<|image_1|>{encoded_image}\\nCarefully analyze this image and describe the context of the photo.\"\n",
    "\n",
    "# Prepare the input data for the model\n",
    "inputs = chat.processor.apply_chat_template([{\"role\": \"user\", \"content\": input_prompt}], images=[], return_tensors=\"pt\")\n",
    "\n",
    "# Run the chain with the processed image\n",
    "result = chain.run(inputs=inputs)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "2ae956553080cfd7",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatNVIDIA' object has no attribute 'processor'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 32\u001B[0m\n\u001B[1;32m     29\u001B[0m input_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<|image_1|>\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mencoded_image\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCarefully analyze this image and describe the context of the photo.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Prepare the input data for the model\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocessor\u001B[49m\u001B[38;5;241m.\u001B[39mapply_chat_template([{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_prompt}], images\u001B[38;5;241m=\u001B[39m[], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Run the chain with the processed image\u001B[39;00m\n\u001B[1;32m     35\u001B[0m result \u001B[38;5;241m=\u001B[39m chain\u001B[38;5;241m.\u001B[39mrun(inputs\u001B[38;5;241m=\u001B[39minputs)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ChatNVIDIA' object has no attribute 'processor'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:30:35.566597Z",
     "start_time": "2024-06-16T05:30:33.240085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import tiktoken\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the system message and human message templates\n",
    "system_message = \"You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images.\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Carefully analyze this image and describe the context of the photo: <image_1/>')\n",
    "\n",
    "# Combine prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using a custom image processor utility\n",
    "def process_image(image_path, max_size=180_000, max_tokens=8100):\n",
    "    encoding = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    # Open the image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        img = Image.open(img_file)\n",
    "\n",
    "        # Function to encode image to base64\n",
    "        def encode_image(image):\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "        # Encode the image\n",
    "        encoded_image = encode_image(img)\n",
    "        encoded_image_tokens = encoding.encode(encoded_image)\n",
    "\n",
    "        # Check if the encoded image size is within the limit\n",
    "        if len(encoded_image) < max_size and len(encoded_image_tokens) < max_tokens:\n",
    "            return encoded_image\n",
    "\n",
    "        # Resize the image until it fits within the size limit\n",
    "        while len(encoded_image) >= max_size or len(encoded_image_tokens) >= max_tokens:\n",
    "            # Calculate the resize factor (reduce size by 10%)\n",
    "            new_width = int(img.width * 0.9)\n",
    "            new_height = int(img.height * 0.9)\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "            encoded_image = encode_image(img)\n",
    "            encoded_image_tokens = encoding.encode(encoded_image)\n",
    "\n",
    "        # Final check\n",
    "        if len(encoded_image) < max_size and len(encoded_image_tokens) < max_tokens:\n",
    "            return encoded_image\n",
    "        else:\n",
    "            raise ValueError(\"Image cannot be resized to fit within the size limit.\")\n",
    "\n",
    "# Process the image\n",
    "image_path = '/Users/ganesh/Downloads/m_resized.jpg'\n",
    "encoded_image = process_image(image_path)\n",
    "\n",
    "# Define the input prompt with image placeholder\n",
    "input_prompt = f\">{encoded_image}\\nTell me what is in this photo\"\n",
    "\n",
    "# Directly run the chain with the processed prompt\n",
    "result = chain.run(image=encoded_image)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "71370326b87287ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, but I can't assist with that.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:31:30.805503Z",
     "start_time": "2024-06-16T05:31:26.954163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model and processor\n",
    "model_id = \"microsoft/phi-3-vision-128k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Define the system message and human message templates\n",
    "system_message = \"You are an advanced AI model with the capability to analyze and describe the context of images as well as understand and respond to text-based queries based on input images.\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Carefully analyze this image and describe the context of the photo: <|image_1|>')\n",
    "\n",
    "# Combine prompts into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Create an LLM chain with the chat model and the prompt\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "# Process the image using a custom image processor utility\n",
    "def process_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return buffered.getvalue()\n",
    "\n",
    "# Process the image\n",
    "image_path = '/Users/ganesh/Downloads/m_resized.jpg'\n",
    "image_data = process_image(image_path)\n",
    "\n",
    "# Prepare the messages with the image\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>Carefully analyze this image and describe the context of the photo.\"}\n",
    "]\n",
    "\n",
    "# Format the input using the processor\n",
    "inputs = processor(messages, images=[image_data], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# Define generation arguments\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Generate the response\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(response)\n",
    "\n"
   ],
   "id": "da87cc62ab8cfe86",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers_modules.microsoft.phi-3-vision-128k-instruct'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Initialize the model and processor\u001B[39;00m\n\u001B[1;32m     15\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicrosoft/phi-3-vision-128k-instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 16\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m processor \u001B[38;5;241m=\u001B[39m AutoProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Define the system message and human message templates\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:551\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m trust_remote_code:\n\u001B[1;32m    550\u001B[0m     class_ref \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mauto_map[\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m]\n\u001B[0;32m--> 551\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m \u001B[43mget_class_from_dynamic_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_ref\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    554\u001B[0m     _ \u001B[38;5;241m=\u001B[39m hub_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(pretrained_model_name_or_path):\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:514\u001B[0m, in \u001B[0;36mget_class_from_dynamic_module\u001B[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001B[0m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# And lastly we get the class inside our newly created module\u001B[39;00m\n\u001B[1;32m    502\u001B[0m final_module \u001B[38;5;241m=\u001B[39m get_cached_module_file(\n\u001B[1;32m    503\u001B[0m     repo_id,\n\u001B[1;32m    504\u001B[0m     module_file \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    512\u001B[0m     repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    513\u001B[0m )\n\u001B[0;32m--> 514\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_class_in_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclass_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:212\u001B[0m, in \u001B[0;36mget_class_in_module\u001B[0;34m(class_name, module_path)\u001B[0m\n\u001B[1;32m    210\u001B[0m     sys\u001B[38;5;241m.\u001B[39mmodules[name] \u001B[38;5;241m=\u001B[39m module\n\u001B[1;32m    211\u001B[0m \u001B[38;5;66;03m# reload in both cases\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m \u001B[43mmodule_spec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexec_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, class_name)\n",
      "File \u001B[0;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-3-vision-128k-instruct/7b92b8c62807f5a98a9fa47cdfd4144f11fbd112/modeling_phi3_v.py:48\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     40\u001B[0m     add_code_sample_docstrings,\n\u001B[1;32m     41\u001B[0m     add_start_docstrings,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m     replace_return_docstrings,\n\u001B[1;32m     46\u001B[0m )\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfiguration_phi3_v\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Phi3VConfig\n\u001B[0;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_embedding_phi3_v\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Phi3ImageEmbedding\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflash_attn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flash_attn_func, flash_attn_varlen_func\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'transformers_modules.microsoft.phi-3-vision-128k-instruct'"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:38:56.697527Z",
     "start_time": "2024-06-16T05:38:56.673723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import base64\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Define the prompts\n",
    "system_prompt = \"You are an AI model specialized in answering questions about images.\"\n",
    "human_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    human_prompt |\n",
    "    chat |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Get the image content\n",
    "image_path = '/Users/ganesh/Downloads/m_resized.jpg'\n",
    "image_content = Image.open(image_path)\n",
    "b64_string = base64.b64encode(image_content).decode('utf-8')\n",
    "\n",
    "# Create the input for the chain\n",
    "input_message = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe this image:\"},\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_string}\"}}\n",
    "]\n",
    "\n",
    "# Run the chain\n",
    "result = chat.invoke([HumanMessage(content=input_message)])\n",
    "print(result.content)\n"
   ],
   "id": "baeb7a25041e5553",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'JpegImageFile'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 31\u001B[0m\n\u001B[1;32m     29\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/ganesh/Downloads/m_resized.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     30\u001B[0m image_content \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\n\u001B[0;32m---> 31\u001B[0m b64_string \u001B[38;5;241m=\u001B[39m \u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mb64encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_content\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Create the input for the chain\u001B[39;00m\n\u001B[1;32m     34\u001B[0m input_message \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     35\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDescribe this image:\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     36\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_url\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_url\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata:image/jpeg;base64,\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mb64_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m}}\n\u001B[1;32m     37\u001B[0m ]\n",
      "File \u001B[0;32m~/miniconda3/envs/vision/lib/python3.10/base64.py:58\u001B[0m, in \u001B[0;36mb64encode\u001B[0;34m(s, altchars)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mb64encode\u001B[39m(s, altchars\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     52\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Encode the bytes-like object s using Base64 and return a bytes object.\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;03m    Optional altchars should be a byte string of length 2 which specifies an\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124;03m    alternative alphabet for the '+' and '/' characters.  This allows an\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    application to e.g. generate url or filesystem safe Base64 strings.\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     encoded \u001B[38;5;241m=\u001B[39m \u001B[43mbinascii\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mb2a_base64\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m altchars \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(altchars) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mrepr\u001B[39m(altchars)\n",
      "\u001B[0;31mTypeError\u001B[0m: a bytes-like object is required, not 'JpegImageFile'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:43:06.427101Z",
     "start_time": "2024-06-16T05:43:02.097088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "# Initialize the ChatNVIDIA model\n",
    "chat = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "\n",
    "# Define the prompts\n",
    "system_prompt = \"You are an AI model specialized in answering questions about images.\"\n",
    "human_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    human_prompt |\n",
    "    chat |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Load the local image\n",
    "image_path = \"/Users/ganesh/Downloads/imgs/class-diagram-example.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Encode the image in base64\n",
    "buffered = io.BytesIO()\n",
    "image.save(buffered, format=\"PNG\")\n",
    "b64_string = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Create the input for the chain\n",
    "input_message = [\n",
    "    {\"type\": \"text\", \"text\": \"Describe this image:\"},\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_string}\"}}\n",
    "]\n",
    "\n",
    "# Run the chain\n",
    "result = chat.invoke([HumanMessage(content=input_message)])\n",
    "print(result.content)\n"
   ],
   "id": "6fc37282f7677e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagram is a UML class diagram with three classes: Person, Address, and two subclasses, Student and Professor, which inherit from Person. The Person class has attributes like name, phoneNumber, emailAddress, and a method purchaseParkingPass(). The Address class has attributes like street, city, state, postalCode, and methods validate() and outputAsLabel(). The Student class has attributes studentNumber, averageMark, and methods isEligibleToEnroll() and getSeminarsTaken(). The Professor class has attributes salary, staffNumber, yearsOfService, and numberOfClasses. There are associations between the classes indicating relationships such as 'lives at' between Person and Address, and 'supervises' between Student and Professor.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c378437401410c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
